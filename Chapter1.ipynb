{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrnSawqLItaTXEhlN/W0ti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajw109/SkdimoreCodes/blob/main/Chapter1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Terms and Definitions**\n",
        "\n",
        "This is a list of the key terms and definitions that I have picked up through my study of the fundamentals of language models and prompt engineering. These definitions were predomenantly derived from the book *Hands-On Large Language Models*.\n",
        "<br>\n",
        "\n",
        "* <u>Language AI</u>: A sub-field of artificial intelligence (AI) that focuses on developing technologies that can of understand, process, and generating human language.  \n",
        "\n",
        "* <u>Natural Language Processing:</u> Can be used interchangeably with Language AI and is abbreviated as \"NLP\".\n",
        "\n",
        "* <u>Large Language Model:</u> A sophisticated AI model within the realm of Language AI, specifically engineered to analyze, and, in some cases generate human language using patterns and insights derived from large amounts of data.\n",
        "\n",
        "* <u>Representation vs Generation Language Models:</u>\n",
        "  * <u>Representation models:</u> Large language models (LLMs) that do not generate text but are commonly used for task-specific use cases, like classification. They focus on creating embeddings, and are referred to as encoder-only models.\n",
        "  * <u>Generation models:</u> LLMS that that generate text, like generative pre-trained transformer (GPT) models. They focus primarily on generating text and are not trained to generate embeddings. Referred to as decoder-only models and completion models.\n",
        "\n",
        "* <u>Embeddings:</u> Vector representations of data that attempt to capture its meaning.\n",
        "\n",
        "* <u>Node:</u> Represents a specific data point or element within the model's architecture, which takes weights, performs calculations, and produces output.\n",
        "\n",
        "* <u>Weights:</u> The strength and direction of the influence one node has on another.\n",
        "\n",
        "* <u>Recurrent Neural Networks (RNNs):</u>\n",
        "  * Variants of neural networds that can model sentences as an additional input.\n",
        "  * RNNS are used for 2 tasks:\n",
        "    * Encoding: representing an input sentence\n",
        "    * Decoding: generating an output sentence\n",
        "  * Autoregressive: each previous output token is used as input to generate the next token.\n",
        "\n",
        "* <u>Attention:</u> Allows a model to focus on parts of the input sequence that are relevant to one another and amplify their signal.\n",
        "\n",
        "* <u>Transformer:</u>\n",
        "  * Soley based on the attention mechanism and removed the recurrence network.\n",
        "  * Trains in parallel.\n",
        "  * Encoder and decoder components are stacked on top of each other.\n",
        "  * Remains autoregressive, needing to consume each generated word before consuming a new word.\n",
        "  * The encoder block consists of two parts:\n",
        "    * Self-attention\n",
        "    * Feedforward neural network\n",
        "  * The decoder has an additional attention layer that attends to the output of the encoder.\n",
        "\n",
        "* <u>Self-attention:</u>\n",
        "  * Can attend to different positions within a single sequence, thereby more easily and accurately representing the input.\n",
        "  * Instad of processing one token at a time, it can be used to look at the entire sequence in one go.\n",
        "\n",
        "* <u>Masked Language Modeling:</u> This method masks a part of the input for the model to predict.\n",
        "\n",
        "* <u>Transfer Learning:</u> Involves using knowledge gained from a pre-trained model, then fine-tuning the model for a specific task. Allows LLMs to efficiently adapt to specific applications.\n",
        "\n",
        "* <u>Generative Pre-trained Transformer (GPT):</u> Uses a decoder-only architecture and removes the encoder-attention block.\n",
        "\n",
        "* <u>Generative LLMS:</u>\n",
        "  * Take in some text, and attempt to complete it.\n",
        "  * Also can be trained as a chatbot.\n",
        "    * Takes in a user query (prompt)\n",
        "    * Outputs a response that would most likely follow that prompt.\n",
        "\n",
        "* <u>Context Window/Length:</u> The maximum number of tokens the model can process.\n",
        "\n",
        "* <u>Foundational Models:</u> Open source based models. Can be fine tuned for specific tasks.\n",
        "\n",
        "* <u>Traditional Machine Learning:</u> Training a model for a specific target task, like classification or regression.\n",
        "\n",
        "* <u>Two-step Approach to Training LLMS:</u>\n",
        "  * Pretraining (Language Modeling)\n",
        "  * Fine-tuning (Specific Task)\n",
        "\n",
        "* <u>Supervised Learning:</u> Uses labeled data, where each input has a corresponding correct output, to train a model to make predictions.\n",
        "\n",
        "* <u>Unsupervised Learning:</u> Uses unlabeled data to discover patterns and relationships without predefined outputs.\n",
        "\n",
        "* <u>Semantic Search:</u> Involves understanding the meaning and intent behind a user's query, rather than just matching keywords. It NLP to analyze the meaning of words and phrases in a search query and find the most relevant results.\n",
        "\n",
        "* <u>Closed Source/Proprietary Models:</u> Owned and controlled by a specific entity, often a company, and their usage is typically governed by licensing terms. Do not have their weights and architecture\n",
        "shared with the public.\n",
        "\n",
        "* <u>Public/Open LLMS:</u> Open for anyone to access, use, modify, and distribute, often under a license that permits these actions. They share their weights and architecture with the public.\n",
        "\n",
        "* <u>API:</u> An API, or Application Programming Interface, defines how different software components or systems can communicate and exchange data or services.\n",
        "\n",
        "* <u>Backend Packages:</u> Packages without a GUI that are created for efficiently loading and running\n",
        "any LLM on your device.\n",
        "\n",
        "**Prompt Engineering**\n",
        "\n",
        "* <u>Temperature:</u>\n",
        "  * Controls the randomness or creativity of the text generated.\n",
        "  * A temperature of 0 generates the same response every time, and a higher vlaue allowing less probable words to be generated.\n",
        "  * High temperature: results in a more diverse output / stochastic behavior.\n",
        "  * Low temperature: creates a more deterministic output.  \n",
        "  <br>\n",
        "\n",
        "### **A Brief Timeline**\n",
        "\n",
        "* <u>Bag-of-Words Model</u>\n",
        "  * A representation model used for representing unstructured text.\n",
        "  * Steps:\n",
        "    * Step 1: Tokenization\n",
        "    * Step 2: Combine all unique words from each sentence to create a vocabulary\n",
        "    * Step 3: Count how often a word in each sentence appears\n",
        "  * Outcome: Creates a representation of text in the form of numbers, also called a vector representation.\n",
        "  * Issue: Ignores the semantic nature of text.\n",
        "\n",
        "* <u>word2vec</u>\n",
        "  * Released in 2013, was one of the first successful attempts at capturing the meaning of text in embeddings.\n",
        "  * Words will always have the same embeddings regardless of the context it is used in, which presents an issue.\n",
        "\n",
        "* <u>BERT:</u>\n",
        "  * Bidirectional Encoder Representations from Transformers.\n",
        "  * Released by Google in 2018.\n",
        "  * Encoder-only architecture that focuses on representing language.\n",
        "  * The encoder blocks are self-attention followed by feedforward neural networks.\n",
        "  * CLS: Classification token, represents the entire input.\n",
        "  * Major advancement: Contextual embeddings that vary depending on the word's usage in a sentence.\n",
        "\n",
        "* <u>GPT (Generative Pre-trained Transformer)</u>\n",
        "  * Decoder-only transformer model released by OpenAI.\n",
        "  * Focuses on text generation tasks.\n",
        "  * Major contribution: Unified pretraining and fine-tuning framework for NLP."
      ],
      "metadata": {
        "id": "4Dr7K90bfjLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generating My First Text**\n",
        "\n",
        "<u>When you use an LLM, two models are loaded:</u>\n",
        "* The generative model itself\n",
        "* Its underlying tokenizer\n",
        "\n",
        "<u>Steps:</u>\n",
        "1. Open Google CoLab.\n",
        "2. Switch your runtime to GPU type T4.\n",
        "  *  Go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4\n",
        "3. Install the dependencies for chapter 1 of *Hands-On Large Language Models*.\n",
        "4. Load the model onto the GPU for faster inference.\n",
        "5. Wrap the model and tokenizer in a pipeline to encapsulate the model, tokenizer, and text generation process into a single function.\n",
        "6. Create a prompt as a user and give it to the model."
      ],
      "metadata": {
        "id": "-ezUNmejhNma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the dependencies for chapter 1\n",
        "\n",
        "%%capture\n",
        "!pip install transformers>=4.40.1 accelerate>=0.27.2"
      ],
      "metadata": {
        "id": "rP-3BTEjhWMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first step is to load our model onto the GPU for faster inference.\n",
        "# Note that we load the model and tokenizer separately (although that isn't always necessary).\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "jkUzfjKuhW4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Although we can now use the model and tokenizer directly,\n",
        "# it's much easier to wrap it in a pipeline object.\n",
        "# transformers.pipeline encapsulates the model, tokenizer, and text\n",
        "# generation process into a single function\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # Setting this to false, the prompt won't be returned, but just the output of the model\n",
        "    return_full_text=False,\n",
        "    # Max number of token the model will generate\n",
        "    max_new_tokens=500,\n",
        "    # By setting to false, the model will always select the next most probable token\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "e_Icg1EShZXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we create our prompt as a user and give it to the model:\n",
        "# The prompt (user input / query)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# Generate output\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "eJi37k1Whbn-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}